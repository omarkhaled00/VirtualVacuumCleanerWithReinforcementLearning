{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f5980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "from pygame.locals import QUIT\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class SimpleGridEnvironment(gym.Env):\n",
    "    def __init__(self, grid_size=(20, 20), cell_size=30, wall_positions=None, point_positions=None):\n",
    "        super(SimpleGridEnvironment, self).__init__()\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.cell_size = cell_size\n",
    "        self.wall_positions = set(wall_positions) if wall_positions is not None else set()\n",
    "        self.point_positions = set(point_positions) if point_positions is not None else set()\n",
    "        self.action_space = spaces.Discrete(4)  # 4 possible actions: 0=up, 1=down, 2=left, 3=right\n",
    "        self.observation_space = spaces.Discrete(np.prod(grid_size))\n",
    "\n",
    "        self.agent_pos = np.array([0, 0])\n",
    "        self.goal_pos = np.array([grid_size[0] - 1, 0])  # Updated goal position\n",
    "        self.max_steps = np.prod(grid_size) * 2\n",
    "\n",
    "        self.current_step = 0\n",
    "\n",
    "        self.q_table = np.zeros((np.prod(grid_size), self.action_space.n))\n",
    "\n",
    "        # Q-learning parameters\n",
    "        self.alpha = 0.1  # Learning rate\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.epsilon = 1.5  # Exploration-exploitation trade-off\n",
    "\n",
    "        # Initialize Pygame\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((grid_size[1] * cell_size, grid_size[0] * cell_size))\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = np.array([0, 0])\n",
    "        self.current_step = 0\n",
    "        self.goal_pos = np.array([self.grid_size[0] - 1, 0])  # Reset goal position to left-down corner\n",
    "        self.wall_positions = set(self.generate_walls())  # Optionally reset walls\n",
    "        self.point_positions = set(self.generate_points())  # Optionally reset points\n",
    "        return self._get_observation()\n",
    "\n",
    "    def generate_walls(self):\n",
    "        # Set predefined wall positions\n",
    "        return {\n",
    "            (0, 5), (1, 5), (2, 5), (5, 5), (5, 4), (5, 3), (5, 2), (5, 1), (5, 0),\n",
    "            (12, 0), (12, 1), (12, 2), (12, 3), (12, 4), (12, 5), (12, 6), (12, 7), (0, 11), (1, 11),\n",
    "            (2, 11), (3, 11), (4, 11), (5, 11), (6, 11), (7, 11), (10, 11), (11, 11),\n",
    "            (12, 11), (13, 11), (14, 11), (15, 11), (16, 11), (17, 11), (18, 11), (19, 11), (20, 11),\n",
    "            (12, 12), (12, 13), (12, 14), (12, 10), (12, 17), (12, 15), (12, 16), (12, 19), (12, 20),\n",
    "            (12, 18), (4, 16), (4, 15), (3, 16), (3, 15), (9, 15), (9, 16), (16, 4), (16, 3), (16, 5),\n",
    "            (17, 4), (17, 3), (17, 5), (9, 15), (9, 16)\n",
    "        }\n",
    "\n",
    "    def generate_points(self):\n",
    "        # Set predefined point positions\n",
    "        return {\n",
    "            (2, 2), (2, 3), (2, 4), (9, 2), (9, 3), (9, 4), (8, 2), (8, 3), (8, 4),\n",
    "            (7, 13), (7, 14), (7, 15), (6, 14), (6, 13), (6, 15), (6, 15), (10, 13),\n",
    "            (15, 1), (15, 2), (14, 1), (14, 2)\n",
    "        }\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action = np.random.choice(self.action_space.n)\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[self._get_observation()])\n",
    "\n",
    "        if action == 0:  # move up\n",
    "            new_pos = (max(0, self.agent_pos[0] - 1), self.agent_pos[1])\n",
    "        elif action == 1:  # move down\n",
    "            new_pos = (min(self.grid_size[0] - 1, self.agent_pos[0] + 1), self.agent_pos[1])\n",
    "        elif action == 2:  # move left\n",
    "            new_pos = (self.agent_pos[0], max(0, self.agent_pos[1] - 1))\n",
    "        elif action == 3:  # move right\n",
    "            new_pos = (self.agent_pos[0], min(self.grid_size[1] - 1, self.agent_pos[1] + 1))\n",
    "\n",
    "        # Check if the new position is a valid move (not a wall)\n",
    "        if new_pos not in self.wall_positions:\n",
    "            self.agent_pos = np.array(new_pos)\n",
    "\n",
    "        # Check if the new position corresponds to a point\n",
    "        if tuple(self.agent_pos) in self.point_positions:  # Convert NumPy array to tuple\n",
    "            self.point_positions.remove(tuple(self.agent_pos))\n",
    "            reward = 10  # Reward for collecting a point\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        done = np.array_equal(self.agent_pos, self.goal_pos) or self.current_step >= self.max_steps\n",
    "\n",
    "        if done:\n",
    "            reward += 1 if np.array_equal(self.agent_pos, self.goal_pos) else 0\n",
    "\n",
    "        # Q-value update\n",
    "        current_q = self.q_table[self._get_observation(), action]\n",
    "        max_future_q = np.max(self.q_table[self._get_observation()])\n",
    "        new_q = (1 - self.alpha) * current_q + self.alpha * (reward + self.gamma * max_future_q)\n",
    "        self.q_table[self._get_observation(), action] = new_q\n",
    "\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return np.ravel_multi_index(self.agent_pos, self.grid_size)\n",
    "\n",
    "    def render(self):\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "\n",
    "        self.screen.fill((255, 255, 255))\n",
    "\n",
    "        agent_rect = pygame.Rect(self.agent_pos[1] * self.cell_size, self.agent_pos[0] * self.cell_size,\n",
    "                                 self.cell_size, self.cell_size)\n",
    "        pygame.draw.rect(self.screen, (0, 0, 255), agent_rect)\n",
    "\n",
    "        goal_rect = pygame.Rect(self.goal_pos[1] * self.cell_size, self.goal_pos[0] * self.cell_size,\n",
    "                                self.cell_size, self.cell_size)\n",
    "        pygame.draw.rect(self.screen, (0, 255, 0), goal_rect)\n",
    "\n",
    "        # Convert the set to a list before iterating over it\n",
    "        wall_positions_list = list(self.wall_positions)\n",
    "        for wall_pos in wall_positions_list:\n",
    "            wall_rect = pygame.Rect(wall_pos[1] * self.cell_size, wall_pos[0] * self.cell_size,\n",
    "                                    self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(self.screen, (150, 150, 150), wall_rect)\n",
    "\n",
    "        # Convert the set to a list before iterating over it\n",
    "        point_positions_list = list(self.point_positions)\n",
    "        for point_pos in point_positions_list:\n",
    "            point_rect = pygame.Rect(point_pos[1] * self.cell_size, point_pos[0] * self.cell_size,\n",
    "                                     self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(self.screen, (255, 0, 0), point_rect)\n",
    "\n",
    "        pygame.display.flip()\n",
    "\n",
    "# Example usage:\n",
    "try:\n",
    "    best_q_table = np.load('best_q_table.npy')\n",
    "except FileNotFoundError:\n",
    "    best_q_table = None\n",
    "\n",
    "# Create the environment\n",
    "env = SimpleGridEnvironment(grid_size=(20, 20), cell_size=30)\n",
    "\n",
    "# Initialize Q-table with the loaded best Q-table or zeros if not available\n",
    "if best_q_table is not None:\n",
    "    env.q_table = best_q_table\n",
    "else:\n",
    "    env.q_table = np.zeros((np.prod(env.grid_size), env.action_space.n))\n",
    "\n",
    "# Training loop\n",
    "for episode in range(10000):\n",
    "    observation = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        #env.render()\n",
    "\n",
    "        if np.random.uniform() < env.epsilon:\n",
    "            action = np.random.choice(env.action_space.n)\n",
    "        else:\n",
    "            action = np.argmax(env.q_table[observation])\n",
    "\n",
    "        next_observation, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        current_q = env.q_table[observation, action]\n",
    "        max_future_q = np.max(env.q_table[next_observation])\n",
    "        new_q = (1 - env.alpha) * current_q + env.alpha * (reward + env.gamma * max_future_q)\n",
    "        env.q_table[observation, action] = new_q\n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "        if done:\n",
    "            if best_q_table is None or total_reward > np.sum(best_q_table):\n",
    "                best_q_table = np.copy(env.q_table)\n",
    "                np.save('best_q_table.npy', best_q_table)\n",
    "\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
    "            break\n",
    "            \n",
    "\n",
    "# Display the best run\n",
    "print(f\"\\nBest Run - Total Reward: {np.sum(best_q_table)}\")\n",
    "env.q_table = best_q_table  # Use the best Q-table for rendering the best run\n",
    "for _ in range(100):  # Run the environment with the best Q-table for 100 steps\n",
    "    env.render()\n",
    "    action = np.argmax(env.q_table[observation])\n",
    "    observation, _, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "pygame.quit()   # Close the Pygame window after displaying the best run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d230eea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.3.0 (SDL 2.24.2, Python 3.9.18)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "\n",
      "Best Run - Total Reward: 517.4003301776922\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "from pygame.locals import QUIT\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class SimpleGridEnvironment(gym.Env):\n",
    "    def __init__(self, grid_size=(20, 20), cell_size=30, wall_positions=None, point_positions=None):\n",
    "        super(SimpleGridEnvironment, self).__init__()\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.cell_size = cell_size\n",
    "        self.wall_positions = set(wall_positions) if wall_positions is not None else set()\n",
    "        self.point_positions = set(point_positions) if point_positions is not None else set()\n",
    "        self.action_space = spaces.Discrete(4)  # 4 possible actions: 0=up, 1=down, 2=left, 3=right\n",
    "        self.observation_space = spaces.Discrete(np.prod(grid_size))\n",
    "\n",
    "        self.agent_pos = np.array([0, 0])\n",
    "        self.goal_pos = np.array([grid_size[0] - 1, 0])  # Updated goal position\n",
    "        self.max_steps = np.prod(grid_size) * 2\n",
    "\n",
    "        self.current_step = 0\n",
    "\n",
    "        self.q_table = np.zeros((np.prod(grid_size), self.action_space.n))\n",
    "\n",
    "        # Q-learning parameters\n",
    "        self.alpha = 0.1  # Learning rate\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.epsilon = 1.5  # Exploration-exploitation trade-off\n",
    "\n",
    "        # Initialize Pygame\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((grid_size[1] * cell_size, grid_size[0] * cell_size))\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = np.array([0, 0])\n",
    "        self.current_step = 0\n",
    "        self.goal_pos = np.array([self.grid_size[0] - 1, 0])  # Reset goal position to left-down corner\n",
    "        self.wall_positions = set(self.generate_walls())  # Optionally reset walls\n",
    "        self.point_positions = set(self.generate_points())  # Optionally reset points\n",
    "        return self._get_observation()\n",
    "\n",
    "    def generate_walls(self):\n",
    "        # Set predefined wall positions\n",
    "        return {\n",
    "            (0, 5), (1, 5), (2, 5), (5, 5), (5, 4), (5, 3), (5, 2), (5, 1), (5, 0),\n",
    "            (12, 0), (12, 1), (12, 2), (12, 3), (12, 4), (12, 5), (12, 6), (12, 7), (0, 11), (1, 11),\n",
    "            (2, 11), (3, 11), (4, 11), (5, 11), (6, 11), (7, 11), (10, 11), (11, 11),\n",
    "            (12, 11), (13, 11), (14, 11), (15, 11), (16, 11), (17, 11), (18, 11), (19, 11), (20, 11),\n",
    "            (12, 12), (12, 13), (12, 14), (12, 10), (12, 17), (12, 15), (12, 16), (12, 19), (12, 20),\n",
    "            (12, 18), (4, 16), (4, 15), (3, 16), (3, 15), (9, 15), (9, 16), (16, 4), (16, 3), (16, 5),\n",
    "            (17, 4), (17, 3), (17, 5), (9, 15), (9, 16)\n",
    "        }\n",
    "\n",
    "    def generate_points(self):\n",
    "        # Set predefined point positions\n",
    "        return {\n",
    "            (2, 2), (2, 3), (2, 4), (9, 2), (9, 3), (9, 4), (8, 2), (8, 3), (8, 4),\n",
    "            (7, 13), (7, 14), (7, 15), (6, 14), (6, 13), (6, 15), (6, 15), (10, 13),\n",
    "            (15, 1), (15, 2), (14, 1), (14, 2)\n",
    "        }\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action = np.random.choice(self.action_space.n)\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[self._get_observation()])\n",
    "\n",
    "        if action == 0:  # move up\n",
    "            new_pos = (max(0, self.agent_pos[0] - 1), self.agent_pos[1])\n",
    "        elif action == 1:  # move down\n",
    "            new_pos = (min(self.grid_size[0] - 1, self.agent_pos[0] + 1), self.agent_pos[1])\n",
    "        elif action == 2:  # move left\n",
    "            new_pos = (self.agent_pos[0], max(0, self.agent_pos[1] - 1))\n",
    "        elif action == 3:  # move right\n",
    "            new_pos = (self.agent_pos[0], min(self.grid_size[1] - 1, self.agent_pos[1] + 1))\n",
    "\n",
    "        # Check if the new position is a valid move (not a wall)\n",
    "        if new_pos not in self.wall_positions:\n",
    "            self.agent_pos = np.array(new_pos)\n",
    "\n",
    "        # Check if the new position corresponds to a point\n",
    "        if tuple(self.agent_pos) in self.point_positions:  # Convert NumPy array to tuple\n",
    "            self.point_positions.remove(tuple(self.agent_pos))\n",
    "            reward = 10  # Reward for collecting a point\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        done = np.array_equal(self.agent_pos, self.goal_pos) or self.current_step >= self.max_steps\n",
    "\n",
    "        if done:\n",
    "            reward += 1 if np.array_equal(self.agent_pos, self.goal_pos) else 0\n",
    "\n",
    "        # Q-value update\n",
    "        current_q = self.q_table[self._get_observation(), action]\n",
    "        max_future_q = np.max(self.q_table[self._get_observation()])\n",
    "        new_q = (1 - self.alpha) * current_q + self.alpha * (reward + self.gamma * max_future_q)\n",
    "        self.q_table[self._get_observation(), action] = new_q\n",
    "\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return np.ravel_multi_index(self.agent_pos, self.grid_size)\n",
    "\n",
    "    def render(self):\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "\n",
    "        self.screen.fill((255, 255, 255))\n",
    "\n",
    "        agent_rect = pygame.Rect(self.agent_pos[1] * self.cell_size, self.agent_pos[0] * self.cell_size,\n",
    "                                 self.cell_size, self.cell_size)\n",
    "        pygame.draw.rect(self.screen, (0, 0, 255), agent_rect)\n",
    "\n",
    "        goal_rect = pygame.Rect(self.goal_pos[1] * self.cell_size, self.goal_pos[0] * self.cell_size,\n",
    "                                self.cell_size, self.cell_size)\n",
    "        pygame.draw.rect(self.screen, (0, 255, 0), goal_rect)\n",
    "\n",
    "        # Convert the set to a list before iterating over it\n",
    "        wall_positions_list = list(self.wall_positions)\n",
    "        for wall_pos in wall_positions_list:\n",
    "            wall_rect = pygame.Rect(wall_pos[1] * self.cell_size, wall_pos[0] * self.cell_size,\n",
    "                                    self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(self.screen, (150, 150, 150), wall_rect)\n",
    "\n",
    "        # Convert the set to a list before iterating over it\n",
    "        point_positions_list = list(self.point_positions)\n",
    "        for point_pos in point_positions_list:\n",
    "            point_rect = pygame.Rect(point_pos[1] * self.cell_size, point_pos[0] * self.cell_size,\n",
    "                                     self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(self.screen, (255, 0, 0), point_rect)\n",
    "\n",
    "        pygame.display.flip()\n",
    "\n",
    "# Example usage:\n",
    "try:\n",
    "    best_q_table = np.load('best_q_table.npy')\n",
    "except FileNotFoundError:\n",
    "    best_q_table = None\n",
    "\n",
    "# Create the environment\n",
    "env = SimpleGridEnvironment(grid_size=(20, 20), cell_size=30)\n",
    "\n",
    "# Initialize Q-table with the loaded best Q-table or zeros if not available\n",
    "if best_q_table is not None:\n",
    "    env.q_table = best_q_table\n",
    "else:\n",
    "    env.q_table = np.zeros((np.prod(env.grid_size), env.action_space.n))\n",
    "\n",
    "# Training loop\n",
    "for episode in range(10):\n",
    "    observation = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        env.render()\n",
    "\n",
    "        if np.random.uniform() < env.epsilon:\n",
    "            action = np.random.choice(env.action_space.n)\n",
    "        else:\n",
    "            action = np.argmax(env.q_table[observation])\n",
    "\n",
    "        next_observation, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        current_q = env.q_table[observation, action]\n",
    "        max_future_q = np.max(env.q_table[next_observation])\n",
    "        new_q = (1 - env.alpha) * current_q + env.alpha * (reward + env.gamma * max_future_q)\n",
    "        env.q_table[observation, action] = new_q\n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "        if done:\n",
    "            if best_q_table is None or total_reward > np.sum(best_q_table):\n",
    "                best_q_table = np.copy(env.q_table)\n",
    "                np.save('best_q_table.npy', best_q_table)\n",
    "\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
    "            break\n",
    "            \n",
    "\n",
    "# Display the best run\n",
    "print(f\"\\nBest Run - Total Reward: {np.sum(best_q_table)}\")\n",
    "env.q_table = best_q_table  # Use the best Q-table for rendering the best run\n",
    "for _ in range(100):  # Run the environment with the best Q-table for 100 steps\n",
    "    env.render()\n",
    "    action = np.argmax(env.q_table[observation])\n",
    "    observation, _, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "pygame.quit()   # Close the Pygame window after displaying the best run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ffa62f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
